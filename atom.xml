<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>just for writing somethings</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://vickyleexy.com/"/>
  <updated>2017-07-13T09:32:05.391Z</updated>
  <id>http://vickyleexy.com/</id>
  
  <author>
    <name>vickyleexy</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>&#39;Basics of Bioinformatics&#39;</title>
    <link href="http://vickyleexy.com/2017/07/13/Basics-of-Bioinformatics/"/>
    <id>http://vickyleexy.com/2017/07/13/Basics-of-Bioinformatics/</id>
    <published>2017-07-13T09:27:09.000Z</published>
    <updated>2017-07-13T09:32:05.391Z</updated>
    
    <content type="html"><![CDATA[<h2 id="生物信息学基础"><a href="#生物信息学基础" class="headerlink" title="生物信息学基础"></a>生物信息学基础</h2><h3 id="lncRNA"><a href="#lncRNA" class="headerlink" title="lncRNA:"></a>lncRNA:</h3><p>　　长链非编码RNA（Long Noncoding RNA，LncRNA）指的是长度在200nt以上、不编码蛋白、但参与细胞内多种生物学过程的RNA分子。人类基因组计划研究发现只有3%的基因组序列是编码蛋白质的基因，而占人基因组62%的序列转录为lncRNA，这一结论提示非编码区域可能通过表达lncRNA，活跃地参与到生物学功能的调控中。在过去的十几年中，科学家们已经相继发现，lncRNA参与了X染色体沉默，染色质修饰、转录激活、转录干扰、核内运输等多种重要的调控过程，截至目前，在NONCODE中已经收录了73370个lncRNA，它们分别来自1239个物种，仅有不到200个进行了功能注释，人类对lncRNA的研究还知之甚少。随着对lncRNA在哺乳动物进化及人类疾病发生发展中作用的日益关注，lncRNA调控机制已成为当前生命科学研究的新热点。</p>
<p><strong>LncRNA在生物体内的功能主要分为三大类：</strong></p>
<p><strong>生物学功能：</strong>LncRNA与表观遗传调控、转录调控、转录后调控、 miRNA 调控、细胞分化及发育等密切相关；</p>
<p><strong>应急功能：</strong>LncRNA可作为细胞内各种信号招募蛋白形成复合物参与免疫反应和宿主防御。</p>
<p><strong>LncRNA与疾病：</strong>LncRNA与人类的许多疾病，尤其是与衰老相关的疾病有密切关系，例如心血管疾病、阿尔兹海默症、糖尿病、癌症等。</p>
<p>因此，lncRNA未来能否作为分子靶标成功应用于临床诊断和癌症治疗，将是其日后发展的难点与热点。</p>
<h3 id="lncRNA-mRNA-整合分析"><a href="#lncRNA-mRNA-整合分析" class="headerlink" title="lncRNA-mRNA 整合分析"></a>lncRNA-mRNA 整合分析</h3><ol>
<li><strong>LncRNA简要：</strong><br>　　LncRNA是一类转录本长度超过200nt的RNA，它们本身并不编码蛋白，而是以RNA的形式在多种层面上（表观遗传调控、转录调控以及转录后调控等）调控基因的表达水平。生物体内含量相相当丰富，约占RNA的4-9%（mRNA约占1-2%）。LncRNA的组织特异性及特定的细胞定位，显示lncRNA受到高度严谨的调控，目前已知其与发育、干细胞维持、癌症及一些疾病相关。虽然近年来随着基因芯片及第二代高通量测序技术的广泛运用，lncRNA不断被发现，但此类转录本的确切功能还未知。目前市场上的lncRNA芯片通常将lncRNA与mRNA设计在一起，RNASeq数据中也包含lncRNA, mRNA序列，因此可以通过分析lncRNA与mRNA表达相关性对lncRNA进行功能注释。</li>
<li><strong>分析流程图：</strong><br><img src="http://i.imgur.com/31PHHQH.jpg" alt=""></li>
<li><strong>分析内容：</strong><br>①计算LncRNA与mRNA表达相关性，根据设定的域值筛选lncRNA与mRNA关系对，构建LncRNA与mRNA共表达网络，如下是全局网络<br><img src="http://img.blog.csdn.net/20170705122150223?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MDM2MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"><br>②基于lncRNA与mRNA表达相关性以及lncRNA与mRNA基因组位置近邻关系，得到lncRNA的潜在靶标基因，对差异表达的lncRNA靶标基因进行功能注释以及功能富集分析，如下是功能富集的GO的Barplot图和差异lncRNA的Heatmap图。<br><img src="http://img.blog.csdn.net/20170705122224470?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MDM2MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"><br><img src="http://img.blog.csdn.net/20170705122236976?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MDM2MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"><br>③研究lncRNA与mRNA的共表达网络的拓扑学特性，基于度筛选网络拓扑上重要的lncRNA，这些lncRNA极有可能是与研究背景相关的lncRNA，如下是重要lncRNA与mRNA的局部共表达子网络。<br><img src="http://img.blog.csdn.net/20170705122329682?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MDM2MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></li>
</ol>
<p>④客户提供研究背景相关一组基因，根据表达相关性可以找出与这组基因相关的lncRNA，从而构建出感兴趣的共表达网络。通过构建的共表达网络能进一步找到感兴趣的 hub  lncRNA。</p>
<p>###<strong>lncRNA深度挖掘分析</strong><br><strong>一、差异lncRNA靶基因预测</strong><br>lncRNA的靶基因较为复杂,主要分为正式和反式两种作用机制.lncRNA作用机制与miRNA类似,均可以通过调控相应的mRNA来行使功能,所以靶基因的预测在科学研究中都显得非常必要。<br><strong>二、靶基因Gene Ontology分析</strong><br>我们将靶基因向gene ontology数据库的各节点映射,计算每个节点的基因数目.<br><strong>三、靶基因Pathway分析</strong><br>信号通路分析需要完备的注释信息支持，通过整合KEGG、Biocarta、Reactome等多个数据库的信息可以精确检验来进行Pathway的显著性分析。<br><strong>四、lncRNA与调控基因的表达机制</strong><br>通过整合lncRNA的信息和靶基因之间的关系,我们可以得到一个lncRNA与靶基因之间的调控网络图.<br><strong>五、 转录因子结合位点预测</strong><br>对于差异表达lncRNA，提取转录起始位点上下游序列,使用预测程序对其转录因子结合位点进行预测.<br><strong>六、基因关联分析</strong><br>现在市面上的lncRNA芯片均含有mRNA的表达探针,通过将lncRNA的靶基因分析结果与芯片上mRNA的表达结果做关联分析,可以更进一步的分析lncRNA的功能。<br><strong>七、信号通路调控网络构建：</strong><br>实验中基因同时参与了很多Pathway，通过构建信号通路调控网络，从宏观层面看到Pathway之间的信号传递关系，在多个显著性Pathway中发现受实验影响的核心Pathway，以及实验影响的信号通路之间的调控机理。<br><strong>八、lncRNA的功能分析</strong><br>根据lncRNA最新的功能数据库，利用生物信息学工具，做出Function-Tar-Net图表，从而得出lncRNA与功能的关系</p>
<p>###<strong>lncRNA功能实验</strong></p>
<ol>
<li>LncRNA定量PCR</li>
<li>LncRNA原位杂交</li>
<li>5’-RACE, 3’-RACE实验(lncRNA全长扩增实验)</li>
<li>lncRNA干扰实验</li>
<li>lncRNA过表达实验<br>需要先通过5’-RACE实验找到lncRNA转录起始位点</li>
<li>lncRNA细胞功能实验<br>细胞增殖、细胞凋亡、细胞周期、细胞迁移</li>
</ol>
<h3 id="基因丰度和基因表达丰度"><a href="#基因丰度和基因表达丰度" class="headerlink" title="基因丰度和基因表达丰度"></a>基因丰度和基因表达丰度</h3><p><strong>基因丰度</strong>是指基因组中该基因的拷贝数量。<br>基因丰度高，即这个基因的数量多，那么可能这个基因的表达量也会多，但是不一定，主要还是要看该基因的启动子强弱。所以基因丰度高不代表表达丰度也高。</p>
<p><strong>基因表达的丰度</strong>是指基因转录成mRNA的数量。<br>基因表达丰度高是指该基因转录成mRNA多，那么表达的蛋白也多，对于表型的影响就大。</p>
<p>(基因丰度是某个基因在基因组中的总数量，其中有的能表达，有的不能表达；而能被表达出来的基因占该基因的总数的比例就是该基因的表达丰度。)</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;生物信息学基础&quot;&gt;&lt;a href=&quot;#生物信息学基础&quot; class=&quot;headerlink&quot; title=&quot;生物信息学基础&quot;&gt;&lt;/a&gt;生物信息学基础&lt;/h2&gt;&lt;h3 id=&quot;lncRNA&quot;&gt;&lt;a href=&quot;#lncRNA&quot; class=&quot;headerlink&quot; 
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>决策树</title>
    <link href="http://vickyleexy.com/2017/06/17/decision_tree/"/>
    <id>http://vickyleexy.com/2017/06/17/decision_tree/</id>
    <published>2017-06-17T04:33:33.383Z</published>
    <updated>2017-07-13T09:17:31.702Z</updated>
    
    <content type="html"><![CDATA[<p>此笔记根据《machine learning in action》和周志华教授的《机器学习》所作。</p>
<h2 id="决策树的原理"><a href="#决策树的原理" class="headerlink" title="决策树的原理"></a>决策树的原理</h2><h3 id="决策树的构造"><a href="#决策树的构造" class="headerlink" title="决策树的构造"></a><strong>决策树的构造</strong></h3><ul>
<li><strong>优点：</strong>计算复杂度不高，输出结果易于理解，对中间值的确实不敏感，可以处理不相关特征数据。</li>
<li><strong>缺点：</strong>可能会产生过度匹配问题。</li>
<li><strong>适用数据类型：</strong>数值型和标称型</li>
</ul>
<a id="more"></a>
<p>《machine learning  in action》:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">If so return 类标签；</div><div class="line">Else</div><div class="line">    寻找划分数据集的最好特征</div><div class="line">    划分数据集</div><div class="line">    创建分支节点</div><div class="line">       for每个划分的子集</div><div class="line">          调用函数createBranch并增加返回结果到分支节点中</div><div class="line">    return 分支节点</div></pre></td></tr></table></figure></p>
<p>上面的伪代码createBranch是一个递归函数，在倒数第二行调用了它自己。<br>《机器学习》：<br><img src="http://img.blog.csdn.net/20170602222239103?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MDM2MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>
<p>###<strong>决策树的一般流程</strong></p>
<ol>
<li><strong>收集数据：</strong>可以使用任何方法。</li>
<li><strong>准备数据：</strong>树构造算法只适用于<strong>标称型数据</strong>（标称型目标变量的结果只在有限目标集中取值，如真与假[标称型目标变量主要用于分类]）,因此数值型数据必须离散化。</li>
<li><strong>分析数据：</strong>可以使用任何方法，构造书完成之后，我们应该检查图形是否符合预期。</li>
<li><strong>训练算法：</strong>构造树的数据结构。</li>
<li><strong>测试算法：</strong>使用经验树计算错误率。</li>
</ol>
<p><strong>决策树的变量可以有两种：</strong></p>
<ol>
<li><p>数字型（Numeric）：变量类型是整数或浮点数，如“年收入”。用“&gt;=”，“&gt;”,“&lt;”或“&lt;=”作为分割条件（排序后，利用已有的分割情况，可以优化分割算法的时间复杂度）。</p>
</li>
<li><p>名称型（Nominal）：类似编程语言中的枚举类型，变量只能重有限的选项中选取，比如“婚姻情况”，只能是“单身”，“已婚”或“离婚”。使用“=”来分割。</p>
</li>
</ol>
<p><strong>一些决策树算法采用二分法划分数据，本文并不采用这种方法，而采用<em>ID3算法</em>。</strong></p>
<h3 id="熵的推导"><a href="#熵的推导" class="headerlink" title="熵的推导"></a><strong>熵的推导</strong></h3><p>参考PRML：<br>熵的含义：<br>考虑一个集合，包含N个完全相同的物体，这些物体要被分到m个箱子里，使得第i个箱子中有$n_i$ 个物体。考虑把物体分配到箱子中的不同方案的数量，有N种方式选择第一个物体，有（N-1）种方式选择第二个物体，以此类推。因此总共有N！种方式把N个物体分配到箱子中（也可以如此考虑：将N个物体先进行全排列，然后选前$n_1$个放入第1个箱子，选前$n_2 $个放入第2个箱子，依次类推，则放入箱子的方案有N！种）。然而，我们并不想区分每个箱子内部物体的排列。所以上述的方案数量需要除以每个箱子内部排列的数量，即在第i个箱子中，有$n_i！$种方案对物体进行全排列，因此，把N个箱子分配到箱子中的总方案数量为：</p>
<p><center><br><img src="http://img.blog.csdn.net/20170602233318436?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MDM2MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="30%" height="30%"><br></center><br>这被称为乘数。<br>熵被定义为通过适当的参数放缩后的对数乘数，即</p>
<p><center><br><img src="http://img.blog.csdn.net/20170602234657912?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MDM2MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="20%" height="20%"><br></center><br>在下面的推导中，将会用到Stirling公式的估计：<br><img src="http://img.blog.csdn.net/20170603110124302?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MDM2MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"><br><img src="http://img.blog.csdn.net/20170603110027505?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MDM2MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"><br><strong>熵的推导：</strong><br><img src="http://img.blog.csdn.net/20170603112104061?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MDM2MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>
<p>决策树算法中比较有代表性的是ID3[Quinlan,1979,1986]，C4.5[Quinlan,1993]和CART[Breiman et al.,1984]</p>
<h3 id="ID3算法"><a href="#ID3算法" class="headerlink" title="ID3算法"></a><strong>ID3算法</strong></h3><p>维基百科解释：<a href="https://en.wikipedia.org/wiki/ID3_algorithm" target="_blank" rel="external">https://en.wikipedia.org/wiki/ID3_algorithm</a><br>ID3是以“信息增益”为准则来选择划分属性的。<br><img src="http://img.blog.csdn.net/20170604211002327?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MDM2MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>
<h3 id="C4-5算法"><a href="#C4-5算法" class="headerlink" title="C4.5算法"></a><strong>C4.5算法</strong></h3><p>C4.5算法主要以“信息率”来选择最优划分属性。<br><img src="http://img.blog.csdn.net/20170604211626464?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MDM2MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"><br><img src="http://img.blog.csdn.net/20170604211640230?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MDM2MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>
<h3 id="CART算法"><a href="#CART算法" class="headerlink" title="CART算法"></a><strong>CART算法</strong></h3><p>CART决策树是以“基尼指数”为准则来选择划分属性。此算法不仅可用于分类还可以用于回归。<br><img src="http://img.blog.csdn.net/20170604212130490?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MDM2MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>
<h2 id="决策树的构建"><a href="#决策树的构建" class="headerlink" title="决策树的构建"></a>决策树的构建</h2><p><strong>《 machine learning in action》之决策树</strong></p>
<h3 id="计算给定数据集的香农熵"><a href="#计算给定数据集的香农熵" class="headerlink" title="计算给定数据集的香农熵"></a><strong>计算给定数据集的香农熵</strong></h3><p>创建文件trees.py<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"># -*- coding=utf-8 -*-</div><div class="line">#计算给定数据集的熵</div><div class="line">from math import log</div><div class="line"></div><div class="line">def calcShannonEnt(dataSet):</div><div class="line">    numEntries = len(dataSet)</div><div class="line">    labelCounts = &#123;&#125;  #实例总数</div><div class="line">    for featVec in dataSet:</div><div class="line">        #为所有可能分类创建字典</div><div class="line">        currentLabel = featVec[-1]       #将最后一列，即分类结果存入currentLabel</div><div class="line">        if currentLabel not in labelCounts.keys():       #若分类结果已经在labelCounts这个字典中</div><div class="line">            labelCounts[currentLabel] = 0              #若当前label不存在,则扩展字典,加入此键值</div><div class="line">        labelCounts[currentLabel] += 1                 #字典中的每个键值都记录了当前类别的数量</div><div class="line">    #print &apos;labelCounts,currentLabel:&apos;,labelCounts,currentLabel</div><div class="line">    #print &apos;labelCounts.keys()&apos;,labelCounts.keys()</div><div class="line">    #print &apos;labelCounts.values()&apos;,labelCounts.values()</div><div class="line">    shannonEnt = 0.0</div><div class="line">    for key in labelCounts:</div><div class="line">        prob = float(labelCounts[key])/numEntries</div><div class="line">        shannonEnt -= prob*log(prob,2)  #求以2为底的对数，其和即为熵</div><div class="line">        #print &apos;labelCounts[key]&apos;,labelCounts[key]</div><div class="line">    return shannonEnt</div></pre></td></tr></table></figure></p>
<h3 id="创建或导入dataSet"><a href="#创建或导入dataSet" class="headerlink" title="创建或导入dataSet"></a><strong>创建或导入dataSet</strong></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">#鱼类数据集</div><div class="line">def createDataSet():</div><div class="line">    dataSet = [[1,1,&apos;yes&apos;],</div><div class="line">               [1,1,&apos;yes&apos;],</div><div class="line">               [1,0,&apos;no&apos;],</div><div class="line">               [0,1,&apos;no&apos;],</div><div class="line">               [0,1,&apos;no&apos;]]</div><div class="line">    labels = [&apos;no surfacing&apos;,&apos;flippers&apos;]</div><div class="line">    return dataSet,labels</div></pre></td></tr></table></figure>
<p><strong>在命令提示符下输入：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">In[9]: import trees</div><div class="line">In[10]: myDat,labels = trees.createDataSet()</div><div class="line">In[11]: myDat</div><div class="line">Out[11]: </div><div class="line">[[1, 1, &apos;yes&apos;], [1, 1, &apos;yes&apos;], [1, 0, &apos;no&apos;], [0, 1, &apos;no&apos;], [0, 1, &apos;no&apos;]]</div><div class="line">In[12]: labels</div><div class="line">Out[12]: </div><div class="line">[&apos;no surfacing&apos;, &apos;flippers&apos;]</div><div class="line">In[13]: trees.calcShannonEnt(myDat)</div><div class="line">#混合的数据越多，熵越高，测试：</div><div class="line">In[18]: myDat[0][-1] = &apos;maybe&apos;</div><div class="line">In[19]: myDat</div><div class="line">Out[19]: </div><div class="line">[[1, 1, &apos;maybe&apos;], [1, 1, &apos;yes&apos;], [1, 0, &apos;no&apos;], [0, 1, &apos;no&apos;], [0, 1, &apos;no&apos;]]</div><div class="line">In[20]: trees.calcShannonEnt(myDat)</div><div class="line">Out[20]: </div><div class="line">1.3709505944546687</div></pre></td></tr></table></figure></p>
<h3 id="划分数据集"><a href="#划分数据集" class="headerlink" title="划分数据集"></a><strong>划分数据集</strong></h3><p>取出符合要求某个特征属性值的样本，并将其此特征从数据集中去除。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">#按照给定特征划分数据集</div><div class="line">def splitDataSet(dataSet,axis,value):  #待划分的数据集,划分数据集的特征(第几列),需要返回的特征的值，（axis--&gt;a,value--&gt;v）</div><div class="line">    #为了不修改原始数据集,创建新的list对象</div><div class="line">    retDataSet = []</div><div class="line">    for featVec in dataSet:</div><div class="line">        #将符合特征的数据抽取出来</div><div class="line">        if featVec[axis] == value:  #Python中的数据.即featVec中的数据从第0列开始,[0,1,2,3,4,5,……]</div><div class="line">            reducedFeatVec = featVec[:axis] #[:axis]表示前axis行,若axis为3,则表示取festVec的前3列,即第[0,1,2]列</div><div class="line">            reducedFeatVec.extend(featVec[axis+1:])    #[axis+1:] 表示跳过axis+1列,从下一列数据取到最后一列的数据,即跳过第[3]列,从第[4]列到最后一列</div><div class="line">            retDataSet.append(reducedFeatVec)</div><div class="line">    return retDataSet</div></pre></td></tr></table></figure></p>
<p><strong>在Python命令行中输入：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">In[16]: reload(trees)</div><div class="line">Out[16]: </div><div class="line">&lt;module &apos;trees&apos; from &apos;/home/vickyleexy/PycharmProjects/Classification of contact lenses/trees.py&apos;&gt;</div><div class="line">In[17]: myDat,labels = trees.createDataSet()</div><div class="line">In[18]: myDat</div><div class="line">Out[18]: </div><div class="line">[[1, 1, &apos;yes&apos;], [1, 1, &apos;yes&apos;], [1, 0, &apos;no&apos;], [0, 1, &apos;no&apos;], [0, 1, &apos;no&apos;]]</div><div class="line">In[19]: trees.splitDataSet(myDat,0,1) #抽取出数据集中第0列中数据值为1的样本，并将此列剔除，组成新的样本集</div><div class="line">Out[19]: </div><div class="line">[[1, &apos;yes&apos;], [1, &apos;yes&apos;], [0, &apos;no&apos;]]</div><div class="line">In[20]: trees.splitDataSet(myDat,0,0) #抽取出数据集中第0列中数据值为0的样本，并将此列剔除，组成新的样本集</div><div class="line">Out[20]: </div><div class="line">[[1, &apos;no&apos;], [1, &apos;no&apos;]]</div><div class="line">In[21]: trees.splitDataSet(myDat,1,0) #抽取出数据集中第1列中数据值为0的样本，并将此列剔除，组成新的样本集</div><div class="line">Out[21]: </div><div class="line">[[1, &apos;no&apos;]]</div></pre></td></tr></table></figure></p>
<h3 id="选取最好的数据集划分方式"><a href="#选取最好的数据集划分方式" class="headerlink" title="选取最好的数据集划分方式"></a><strong>选取最好的数据集划分方式</strong></h3><p>即选取取出信息增益最大的特征。<strong>信息增益</strong>是熵减少或者说信息无序度的减少，信息增益越大，信息无序度减少越大，信息的可确定性越强。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">#选择最好的数据集划分数据,即计算每种特征信息熵 Gain(D,a)</div><div class="line">def chooseBestFeatureToSplit(dataSet):</div><div class="line">    numFeatures = len(dataSet[0]) - 1                     #可选特征的个数</div><div class="line">    baseEntropy = calcShannonEnt(dataSet)  # 计算总体信息熵Ent(D)</div><div class="line">    bestInfoGain = 0.0;       #初始化最好的信息增益</div><div class="line">    bestFeature = -1;        #初始化选取的最好的特征</div><div class="line">    #创建唯一的分类标签列表</div><div class="line">    for i in range(numFeatures):           #遍历各个特征</div><div class="line">        featList = [example[i] for example in dataSet]   #将第i列特征的值选取出来,并存入featList</div><div class="line">        uniqueVals = set(featList)         #将featList存为集合的格式,即去除featList中重复的元素,因此uniqueVals中为每个特征的中不同的属性</div><div class="line">        newEntropy = 0.0</div><div class="line">        #计算每种划分方式的信息熵</div><div class="line">        for value in uniqueVals:        #遍历每个特征中的各个属性</div><div class="line">            subDataSet = splitDataSet(dataSet,i,value)       #选取符合条件的特征,并将此特征从样本中去除，以便进行下面的进一步计算</div><div class="line">            prob = len(subDataSet)/float(len(dataSet))       #符合此特征中此属性的个数占总体样本的比例,即|D^v|/|D|</div><div class="line">            newEntropy += prob * calcShannonEnt(subDataSet)  #计算各个特征中每个属性的加权信息熵的和</div><div class="line">        infoGain = baseEntropy - newEntropy               #计算信息增益,即Gain(D,a)</div><div class="line">        #计算最好额信息增益</div><div class="line">        if (infoGain &gt; bestInfoGain):</div><div class="line">            bestInfoGain = infoGain</div><div class="line">            bestFeature = i</div><div class="line">    print &apos;最好的特征,最好的信息增益：&apos;,bestFeature,bestInfoGain</div><div class="line">    return bestFeature</div></pre></td></tr></table></figure></p>
<p><strong>在Python命令行下输入以下代码进行测试：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">In[9]: reload(trees)</div><div class="line">Out[9]: </div><div class="line">&lt;module &apos;trees&apos; from &apos;/home/vickyleexy/PycharmProjects/Classification of contact lenses/trees.py&apos;&gt;</div><div class="line">In[10]: myDat,labels = trees.createDataSet()</div><div class="line">In[11]: myDat</div><div class="line">Out[11]: </div><div class="line">[[1, 1, &apos;yes&apos;], [1, 1, &apos;yes&apos;], [1, 0, &apos;no&apos;], [0, 1, &apos;no&apos;], [0, 1, &apos;no&apos;]]</div><div class="line">In[12]: trees.chooseBestFeatureToSplit(myDat)</div><div class="line">最好的特征,最好的信息增益： 0 , 0.419973094022</div><div class="line">Out[12]: </div><div class="line">0</div></pre></td></tr></table></figure></p>
<h3 id="递归构建决策树"><a href="#递归构建决策树" class="headerlink" title="递归构建决策树"></a><strong>递归构建决策树</strong></h3><p>递归结束的条件：</p>
<ul>
<li>程序遍历完所有划分数据集的属性</li>
<li>每个分支下的所有实例都具有相同的分类</li>
</ul>
<p>若已经处理了所有的属性,但类标签依然不是唯一的,此时采用多数表决的方法决定叶子节点的分类。<br>在trees.py文件的顶部添加operator库<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">import operator</div></pre></td></tr></table></figure></p>
<p>统计剩余样本中属于哪一类别最多的数量最多：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">def majorityCnt(classList):     #classList类别列表</div><div class="line">    classCount = &#123;&#125;             #新建立一个字典</div><div class="line">    for vote in classList:       #遍历所有的类别</div><div class="line">        #统计各类别剩余样本的数量</div><div class="line">        if vote not in classCount.keys():     #如果此类别不在classCount的key中</div><div class="line">            classCount[vote] = 0</div><div class="line">            classCount[vote] += 1</div><div class="line">    sortedClassCount = sorted(classCount.iteritems(),key = operator.itemgetter(1),reverse = True) #将统计好的剩余样本类别和其数量，根据其数量进行从大到小的排序</div><div class="line">    return sortedClassCount[0][0]     #取出剩余样本中数量最大的类别名称</div></pre></td></tr></table></figure></p>
<p><strong>创建树的函数代码：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">#创建树的函数代码</div><div class="line">def createTree(dataSet,labels):</div><div class="line">    classList = [example[-1] for example in dataSet]</div><div class="line">    #类别完全相同则停止继续划分</div><div class="line">    if classList.count(classList[0]) == len(classList):  #若classList中第一个类别的数量等于样本的总数量,即样本类别完全相同</div><div class="line">        return classList[0]</div><div class="line">    #遍历完所有特征时返回出现次数最多的</div><div class="line">    if len(dataSet[0]) == 1:</div><div class="line">        return majorityCnt(classList)</div><div class="line">    bestFeat = chooseBestFeatureToSplit(dataSet)</div><div class="line">    bestFeatLabel = labels[bestFeat]</div><div class="line">    myTree = &#123;bestFeatLabel:&#123;&#125;&#125;</div><div class="line">    del(labels[bestFeat]) #去掉已经使用的（bestFeat）类标签</div><div class="line">    #得到最好的特征这一列,包含其所有属性值的列表</div><div class="line">    featValues = [example[bestFeat] for example in dataSet]</div><div class="line">    uniqueVals = set(featValues)        #去掉featValues列表中重复的属性</div><div class="line">    for value in uniqueVals:             #遍历最好的特征中所有的属性</div><div class="line">        subLabels = labels[:]            #复制del(labels[bestFeat])后的结果,并将其存在新列表变量subLabels中</div><div class="line">        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet,bestFeat,value),subLabels)</div><div class="line">    return myTree</div></pre></td></tr></table></figure></p>
<p>命令行中输入：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">In[66]: reload(trees)</div><div class="line">Out[66]: </div><div class="line">&lt;module &apos;trees&apos; from &apos;/home/vickyleexy/PycharmProjects/Classification of contact lenses/trees.py&apos;&gt;</div><div class="line">In[67]: myDat,labels = trees.createDataSet()</div><div class="line">In[68]: myTree = trees.createTree(myDat,labels)</div><div class="line">最好的特征,最好的信息增益： 0 , 0.419973094022</div><div class="line">最好的特征,最好的信息增益： 0 , 0.918295834054</div><div class="line">In[69]: myTree</div><div class="line">Out[69]: </div><div class="line">&#123;&apos;no surfacing&apos;: &#123;0: &apos;no&apos;, 1: &#123;&apos;flippers&apos;: &#123;0: &apos;no&apos;, 1: &apos;yes&apos;&#125;&#125;&#125;&#125;</div></pre></td></tr></table></figure></p>
<h2 id="使用matplotlib绘制树形图并测试算法"><a href="#使用matplotlib绘制树形图并测试算法" class="headerlink" title="使用matplotlib绘制树形图并测试算法"></a>使用matplotlib绘制树形图并测试算法</h2><p>在<a href="http://blog.csdn.net/u012150360/article/details/72868619" target="_blank" rel="external"><strong>决策树02——决策树的构建</strong></a>中，我们将已经进行分类的数据存储在字典中，然而字典的表示形式非常不直观，也不容易理解，所以我们将字典中的信息绘制成树形图。</p>
<h3 id="Matplotlib注解功能"><a href="#Matplotlib注解功能" class="headerlink" title="Matplotlib注解功能"></a><strong>Matplotlib注解功能</strong></h3><p>　　Matplotlib提供一个注解工具annotations，它可以在数据图形上添加文本注释。</p>
<p>　　以下将使用Matplotlib的注解功能绘制树形图，它可以对文字着色，并提供多种形状以供选择，而且我们还可以反转箭头，将它指向文本框而不是数据点。</p>
<p>　　新建名为treeplotter.py的新文件，将输入下面的程序代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"># -*-coding=utf-8 -*-</div><div class="line"></div><div class="line">#使用文本朱姐绘制树节点</div><div class="line">import matplotlib.pyplot as plt</div><div class="line"></div><div class="line">#定义文本框和箭头格式</div><div class="line">#定义决策树决策结果的属性（决策节点or叶节点）,用字典来定义</div><div class="line">#下面的字典定义也可以写作 decisionNode = &#123;boxstyle：’sawtooth‘,fc=’0.8‘&#125;</div><div class="line">decisionNode = dict(boxstyle = &quot;sawtooth&quot;, fc = &quot;0.8&quot;)       #决策节点,boxstyle为文本框类型,sawtooth是锯齿形,fc是边框内填充的颜色</div><div class="line">leafNode = dict(boxstyle = &quot;round&quot;,fc=&quot;0.8&quot;)                #叶节点,定义决策树的叶子结点的描述属性</div><div class="line">arrow_args = dict(arrowstyle = &quot;&lt;-&quot;)                         #箭头格式</div><div class="line"></div><div class="line">#绘制带箭头的注释</div><div class="line">def plotNode(nodeTxt,centerPt,parentPt,nodeType):           #nodeTxt是显示的文本,centerPt是文本的中心点,parentPt是箭头的起点坐标,nodeType是一个字典 注解的形状</div><div class="line">    createPlot.ax1.annotate(nodeTxt,xy = parentPt, xycoords = &apos;axes fraction&apos;,  #xy为箭头的起始坐标,0,0 is lower left of axes and 1,1 is upper right</div><div class="line">                            xytext = centerPt,textcoords = &apos;axes fraction&apos;, #xytext为注解内容的坐标</div><div class="line">                            va = &quot;center&quot;,ha = &quot;center&quot;,bbox = nodeType,arrowprops = arrow_args) #bbox注解文本框的形状,arrowprops是指箭头的形状</div><div class="line"></div><div class="line">def createPlot():</div><div class="line">    fig = plt.figure(1,facecolor=&apos;white&apos;)  #类似于matlab的figure,定义一个画布,其背景为白色</div><div class="line">    fig.clf()                 #把画布清空</div><div class="line">    createPlot.ax1 = plt.subplot(111,frameon=False) # createPlot.ax1为全局变量,绘制图像的句柄,subplot为定义了一个绘图,111表示figure中的图有1行1列,即1个,最后的1代表第一个图,</div><div class="line">    plotNode(U&apos;决策节点&apos;,(0.5,0.1),(0.1,0.5), decisionNode)</div><div class="line">    plotNode(U&apos;叶节点&apos;,(0.8,0.1),(0.3,0.8), leafNode)</div><div class="line">    plt.show()</div></pre></td></tr></table></figure></p>
<p><strong>注意：以上程序运行时会出现中文变成小方框的现象，将以下几行代码添加到文件的开始处。</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">from pylab import *</div><div class="line">mpl.rcParams[&apos;font.sans-serif&apos;] = [&apos;SimHei&apos;]  #指定默认字体</div><div class="line">mpl.rcParams[&apos;axes.unicode_minus&apos;] = False</div></pre></td></tr></table></figure></p>
<p><strong>在命令行输入：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">In[70]: import treePlotter</div><div class="line">Backend TkAgg is interactive backend. Turning interactive mode on.</div><div class="line">In[71]: treePlotter.createPlot()</div></pre></td></tr></table></figure></p>
<p><img src="http://img.blog.csdn.net/20170614231531328?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MDM2MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>
<h3 id="构造注解树"><a href="#构造注解树" class="headerlink" title="构造注解树"></a>构造注解树</h3><p>　　我们虽然有x, y坐标，但是如何放置所有的树节点却是个问题。我们必须知道有多少个叶节点，以便可以正确确定x轴的长度，我们还需要知道树有多少层，以便可以正确的确定y轴的高度。<br>　　这里我们定义两个新函数getNumLeafs()和getTreeDepth()，来获取叶节点的输煤和树的层数。将下面的两个函数添加到treePlotter.py文件中。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">#获取叶节点的数目和树的层次</div><div class="line">def getNumLeafs(myTree):</div><div class="line">    numLeaf = 0</div><div class="line">    firstStr = myTree.keys()[0]</div><div class="line">    secondDict = myTree[firstStr]</div><div class="line">    for key in secondDict.keys():</div><div class="line">        if type(secondDict[key]).__name__ ==&apos;dict&apos;:         #测试节点的数据类型是否为字典 ,type(secondDict[key]) ==dict 也是可以的</div><div class="line">            numLeaf += getNumLeafs(secondDict[key])</div><div class="line">        else: numLeaf += 1</div><div class="line">    return numLeaf</div><div class="line"></div><div class="line">def getTreeDepth(myTree):</div><div class="line">    maxDepth = 0</div><div class="line">    firstStr = myTree.keys()[0]</div><div class="line">    secondDict = myTree[firstStr]</div><div class="line">    for key in secondDict.keys():</div><div class="line">        if type(secondDict[key]).__name__ == &apos;dict&apos;:</div><div class="line">            thisDepth = 1 + getTreeDepth(secondDict[key])</div><div class="line">        else: thisDepth = 1</div><div class="line">        if thisDepth &gt; maxDepth :maxDepth = thisDepth</div><div class="line">    return maxDepth</div></pre></td></tr></table></figure></p>
<p>函数retrieveTree()输出预先存储的树信息，将 下面代码添加到文件treePlotter.py中：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">def retrieveTree(i):</div><div class="line">    listOfTrees = [&#123;&apos;no surfacing&apos;:&#123;0:&apos;0&apos;,1:&#123;&apos;flippers&apos;:&#123;0:&apos;no&apos;,1:&apos;yes&apos;&#125;&#125;&#125;&#125;,</div><div class="line">                   &#123;&apos;no surfacing&apos;: &#123;0: &apos;0&apos;, 1: &#123;&apos;flippers&apos;: &#123;0: &#123;&apos;head&apos;:&#123;0:&apos;no&apos;,1:&apos;yes&apos;&#125;&#125;, 1: &apos;no&apos;&#125;&#125;&#125;&#125;</div><div class="line">                   ]</div><div class="line">    return listOfTrees[i]</div></pre></td></tr></table></figure></p>
<p><strong>在命令行中输入：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">In[2]: import treePlotter</div><div class="line">Backend TkAgg is interactive backend. Turning interactive mode on.</div><div class="line">In[3]: treePlotter.retrieveTree(0)</div><div class="line">Out[3]: </div><div class="line">&#123;&apos;no surfacing&apos;: &#123;0: &apos;0&apos;, 1: &#123;&apos;flippers&apos;: &#123;0: &apos;no&apos;, 1: &apos;yes&apos;&#125;&#125;&#125;&#125;</div><div class="line">In[4]: myTree = treePlotter.retrieveTree(0)</div><div class="line">In[5]: treePlotter.getNumLeafs(myTree)</div><div class="line">Out[5]: </div><div class="line">3</div><div class="line">In[6]: treePlotter.getTreeDepth(myTree)</div><div class="line">Out[6]: </div><div class="line">2</div></pre></td></tr></table></figure></p>
<p>将下面代码添加到treePlotter.py中，注意前面已经定义了createPlot()，此时我们需要更新前面的代码。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line">#plotTree函数</div><div class="line">#在父子节点间填充文本信息</div><div class="line">def plotMidText(cntrPt,parentPt,txtString):</div><div class="line">    xMid = (parentPt[0] - cntrPt[0])/2.0 + cntrPt[0]</div><div class="line">    yMid = (parentPt[1] - cntrPt[1])/2.0 + cntrPt[1]</div><div class="line">    createPlot.ax1.text(xMid,yMid,txtString)</div><div class="line"></div><div class="line">#自顶向下作图,绘制图形的x轴有效范围是0.0～1.0, y轴有效范围也是0.0~1.0</div><div class="line">def plotTree(myTree,parentPt,nodeTxt):</div><div class="line">    numLeafs = getNumLeafs(myTree)    #secondDict[key]的叶节点的数量</div><div class="line">    depth = getTreeDepth(myTree)      #secondDict[key]的树深度</div><div class="line">    print &apos;numLeafs,depth:&apos;,numLeafs,&apos;,&apos;,depth</div><div class="line">    firstStr = myTree.keys()[0]</div><div class="line">    # 全局变量plotTree.totalW 存储树的宽度，全局变量PlotTree.totalD 存储树的深度，使用这两个变量计算树节点的摆放位置，这样可以将树绘制在水平方向和垂直方向的中心位置。</div><div class="line">    cntrPt = (plotTree.xOff +(1.0 + float(numLeafs))/2.0/plotTree.totalW,plotTree.yOff) #注释1</div><div class="line">    #标记子节点属性</div><div class="line">    plotMidText(cntrPt,parentPt,nodeTxt)        #这一次循环中的cntrPt(即上式)为cbtrPt,parentPt为上一轮计算出的cntrPt</div><div class="line">    plotNode(firstStr,cntrPt,parentPt,decisionNode)  #因还没画到叶节点,所以这里画的是决策节点,即此时筛选secondDict[key]还是字典</div><div class="line">    secondDict = myTree[firstStr]</div><div class="line">    #计算下一轮要用的y</div><div class="line">    plotTree.yOff = plotTree.yOff - 1.0/plotTree.totalD    #下面的循环中要使用的y</div><div class="line">    for key in secondDict.keys():</div><div class="line">        if type(secondDict[key]).__name__ == &apos;dict&apos;:</div><div class="line">            plotTree(secondDict[key],cntrPt,str(key))</div><div class="line">        else:</div><div class="line">            plotTree.xOff = plotTree.xOff + 1.0/plotTree.totalW</div><div class="line">            plotNode(secondDict[key],(plotTree.xOff,plotTree.yOff),cntrPt,leafNode)</div><div class="line">            plotMidText((plotTree.xOff,plotTree.yOff),cntrPt,str(key))</div><div class="line">    plotTree.yOff = plotTree.yOff + 1.0/plotTree.totalD #注释2</div><div class="line"></div><div class="line">def createPlot(inTree):</div><div class="line">    fig = plt.figure(1, facecolor=&apos;white&apos;)</div><div class="line">    fig.clf()</div><div class="line">    axprops = dict(xticks=[],yticks=[])    #创建一个型为&#123;&apos;xticks&apos;: [], &apos;yticks&apos;: []&#125;的字典</div><div class="line">    createPlot.ax1 = plt.subplot(111,frameon=False,**axprops)</div><div class="line">    plotTree.totalW = float(getNumLeafs(inTree))      #树的宽度</div><div class="line">    plotTree.totalD = float(getTreeDepth(inTree))     #输的深度</div><div class="line">    plotTree.xOff = -0.5/plotTree.totalW; plotTree.yOff = 1.0; #定义节点位置的初始值</div><div class="line">    plotTree(inTree,(0.5,1.0),&apos;&apos;)     #（0.5,1.0）为初始化parentPt的值，注释3</div><div class="line">    plt.show()</div></pre></td></tr></table></figure></p>
<p>在命令行输入：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">In[35]: reload(treePlotter)</div><div class="line">Out[35]: </div><div class="line">&lt;module &apos;treePlotter&apos; from &apos;/home/vickyleexy/PycharmProjects/Classification of contact lenses/treePlotter.py&apos;&gt;</div><div class="line">In[36]: myTree = treePlotter.retrieveTree(0)</div><div class="line">In[37]: myTree</div><div class="line">Out[37]: </div><div class="line">&#123;&apos;no surfacing&apos;: &#123;0: &apos;no&apos;, 1: &#123;&apos;flippers&apos;: &#123;0: &apos;no&apos;, 1: &apos;yes&apos;&#125;&#125;&#125;&#125;</div><div class="line">In[38]: treePlotter.createPlot(myTree)</div><div class="line">numLeafs,depth: 3 , 2</div><div class="line">numLeafs,depth: 2 , 1</div></pre></td></tr></table></figure></p>
<p><img src="http://img.blog.csdn.net/20170615230521666?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MDM2MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>
<p><strong>注释：</strong><br><strong>1.</strong><code>cntrPt = (plotTree.xOff + (1.0 + float(numLeafs))/2.0/plotTree.totalW, plotTree.yOff)</code><br>　　在这行代码中，首先由于整个画布根据叶子节点数和深度进行平均切分，并且x轴的总长度为1,即如同下图：<br><img src="http://img.blog.csdn.net/20170615230919401?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MDM2MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"><br>　　其中方形为非叶子节点的位置，@是叶子节点的位置，因此每份即上图的一个表格的长度应该为1/plotTree.totalW,但是叶子节点的位置应该为@所在位置，则在开始的时候plotTree.xOff的赋值为-0.5/plotTree.totalW,即意为开始x位置为第一个表格左边的半个表格距离位置，这样作的好处为：在以后确定@位置时候可以直接加整数倍的1/plotTree.totalW,</p>
<p> 　　plotTree.xOff即为最近绘制的一个叶子节点的x坐标，在确定当前节点位置时每次只需确定当前节点有几个叶子节点，因此其叶子节点所占的总距离就确定了即为float(numLeafs)/plotTree.totalW<em>1(因为总长度为1)，因此当前节点的位置即为其所有叶子节点所占距离的中间即一半为float(numLeafs)/2.0/plotTree.totalW</em>1，但是由于开始plotTree.xOff赋值并非从0开始，而是左移了半个表格，因此还需加上半个表格距离即为1/2/plotTree.totalW<em>1,则加起来便为(1.0 + float(numLeafs))/2.0/plotTree.totalW</em>1，因此偏移量确定，则x位置变为plotTree.xOff + (1.0 + float(numLeafs))/2.0/plotTree.totalW.</p>
<p><strong>2.</strong> <code>plotTree.yOff = plotTree.yOff + 1.0/plotTree.totalD</code><br>这行代码中是需要的，当分支最后一个不是字典的时候,字典循环完需要返回上一层继续进行函数<br>例如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">In[40]: myTree[&apos;no surfacing&apos;][3] = &apos;maybe&apos;</div><div class="line">In[41]: myTree</div><div class="line">Out[41]: </div><div class="line">&#123;&apos;no surfacing&apos;: &#123;0: &apos;no&apos;, 1: &#123;&apos;flippers&apos;: &#123;0: &apos;no&apos;, 1: &apos;yes&apos;&#125;&#125;, 3: &apos;maybe&apos;&#125;&#125;</div><div class="line">In[42]: treePlotter.createPlot(myTree)</div><div class="line">numLeafs,depth: 4 , 2</div><div class="line">numLeafs,depth: 2 , 1</div></pre></td></tr></table></figure></p>
<p><img src="http://img.blog.csdn.net/20170615231547780?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjE1MDM2MA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"><br>    　<br><strong>3.</strong><code>plotTree(inTree,(0.5,1.0),&#39;&#39;)</code><br>  在这行代码中，对于plotTree函数参数赋值为(0.5, 1.0)，因为开始的根节点并不用划线，因此父节点和当前节点的位置需要重合，利用2中的确定当前节点的位置便为(0.5, 1.0)</p>
<p><strong>总结：</strong>利用这样的逐渐增加x的坐标，以及逐渐降低y的坐标能能够很好的将树的叶子节点数和深度考虑进去，因此图的逻辑比例就很好的确定了，这样不用去关心输出图形的大小，一旦图形发生变化，函数会重新绘制，但是假如利用像素为单位来绘制图形，这样缩放图形就比较有难度了</p>
<h3 id="测试和存储分类器"><a href="#测试和存储分类器" class="headerlink" title="测试和存储分类器"></a>测试和存储分类器</h3><p>程序比较测试数据与决策树上的数值，递归执行该过程直到进入叶子节点，最后将测试数据定义为叶子节点所属的类型。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">#使用决策树的分类算法</div><div class="line">def classify(inputTree,featLabels,testVec):    #testVec即为需要分类的数据</div><div class="line">    firstStr = inputTree.keys()[0]</div><div class="line">    secondDict = inputTree[firstStr]</div><div class="line">    featIndex = featLabels.index(firstStr)        #将标签字符串转换为索引</div><div class="line">    print featIndex</div><div class="line">    for key in secondDict.keys():</div><div class="line">        if testVec[featIndex] == key:</div><div class="line">            if type(secondDict[key]).__name__ == &apos;dict&apos;:</div><div class="line">                classLabel = classify(secondDict[key],featLabels,testVec)</div><div class="line">            else:</div><div class="line">                classLabel = secondDict[key]</div><div class="line">    return classLabel</div></pre></td></tr></table></figure>
<p><strong>在命令行输入：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">In[19]: reload(trees)</div><div class="line">Out[19]: </div><div class="line">&lt;module &apos;trees&apos; from &apos;/home/vickyleexy/PycharmProjects/Classification of contact lenses/trees.py&apos;&gt;</div><div class="line">In[20]: myDat,labels = trees.createDataSet()</div><div class="line">In[21]: myDat</div><div class="line">Out[21]: </div><div class="line">[[1, 1, &apos;yes&apos;], [1, 1, &apos;yes&apos;], [1, 0, &apos;no&apos;], [0, 1, &apos;no&apos;], [0, 1, &apos;no&apos;]]</div><div class="line">In[22]: labels</div><div class="line">Out[22]: </div><div class="line">[&apos;no surfacing&apos;, &apos;flippers&apos;]</div><div class="line">In[23]: myTree = trees.createTree(myDat,labels)</div><div class="line">最好的特征,最好的信息增益： 0 , 0.419973094022</div><div class="line">最好的特征,最好的信息增益： 0 , 0.918295834054</div><div class="line">In[24]: myDat</div><div class="line">Out[24]: </div><div class="line">[[1, 1, &apos;yes&apos;], [1, 1, &apos;yes&apos;], [1, 0, &apos;no&apos;], [0, 1, &apos;no&apos;], [0, 1, &apos;no&apos;]]</div><div class="line">In[25]: labels</div><div class="line">Out[25]: </div><div class="line">[&apos;flippers&apos;]</div><div class="line">In[26]: myDat,labels = trees.createDataSet()</div><div class="line">In[27]: trees.classify(myTree,labels,[1,1])</div><div class="line">0</div><div class="line">1</div><div class="line">Out[27]: </div><div class="line">&apos;yes&apos;</div><div class="line">In[28]: trees.classify(myTree,labels,[1,0])</div><div class="line">0</div><div class="line">1</div><div class="line">Out[28]: </div><div class="line">&apos;no&apos;</div></pre></td></tr></table></figure></p>
<h3 id="决策树的存储"><a href="#决策树的存储" class="headerlink" title="决策树的存储"></a>决策树的存储</h3><p>为了节省时间，最好能够在每次执行分类时调用已经构造好的决策树，使用Python的pickle模块可以在磁盘上保存对象，并在需要的时候读取出来。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">#使用pickle模块存储决策树</div><div class="line">def storeTree(inputTree,filename):</div><div class="line">    import pickle</div><div class="line">    fw = open(filename,&apos;w&apos;)</div><div class="line">    pickle.dump(inputTree,fw)</div><div class="line">    fw.close()</div><div class="line"></div><div class="line">def grabTree(filename):</div><div class="line">    import pickle</div><div class="line">    fr = open(filename)</div><div class="line">    return pickle.load(fr)</div></pre></td></tr></table></figure></p>
<p>在命令行中输入:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">In[29]: trees.storeTree(myTree,&apos;classifierStorage.txt&apos;)</div><div class="line">In[30]: trees.grabTree(&apos;classifierStorage.txt&apos;)</div><div class="line">Out[30]: </div><div class="line">&#123;&apos;no surfacing&apos;: &#123;0: &apos;no&apos;, 1: &#123;&apos;flippers&apos;: &#123;0: &apos;no&apos;, 1: &apos;yes&apos;&#125;&#125;&#125;&#125;</div></pre></td></tr></table></figure></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此笔记根据《machine learning in action》和周志华教授的《机器学习》所作。&lt;/p&gt;
&lt;h2 id=&quot;决策树的原理&quot;&gt;&lt;a href=&quot;#决策树的原理&quot; class=&quot;headerlink&quot; title=&quot;决策树的原理&quot;&gt;&lt;/a&gt;决策树的原理&lt;/h2&gt;&lt;h3 id=&quot;决策树的构造&quot;&gt;&lt;a href=&quot;#决策树的构造&quot; class=&quot;headerlink&quot; title=&quot;决策树的构造&quot;&gt;&lt;/a&gt;&lt;strong&gt;决策树的构造&lt;/strong&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;优点：&lt;/strong&gt;计算复杂度不高，输出结果易于理解，对中间值的确实不敏感，可以处理不相关特征数据。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;缺点：&lt;/strong&gt;可能会产生过度匹配问题。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;适用数据类型：&lt;/strong&gt;数值型和标称型&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://vickyleexy.com/2017/05/27/hello-world/"/>
    <id>http://vickyleexy.com/2017/05/27/hello-world/</id>
    <published>2017-05-27T15:50:27.580Z</published>
    <updated>2017-05-27T15:50:27.596Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
    
    </summary>
    
    
  </entry>
  
</feed>
